[project]
name = "flash-attention-cute"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "torch>=2.0",
    "ninja",
    "pytest",
]

[project.optional-dependencies]
bench = [
    "flash-attn>=2.5",
]

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "--tb=short -q"
markers = [
    "slow: marks tests as slow",
    "cuda: marks tests requiring CUDA",
]
